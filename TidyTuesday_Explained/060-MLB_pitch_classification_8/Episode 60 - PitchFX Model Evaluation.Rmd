---
title: "pitchf/x model evaluation"
author: "TidyX"
date: "4/21/2021"
output: html_document
---

**Data courtest of {mlbgameday}:** https://github.com/keberwein/mlbgameday
**Resource for understanding pitchf/x data:** https://library.fangraphs.com/misc/pitch-fx/
**Resource on the features in the pitchf/x data:** Kagan, David. (2008). Fun with PitchFX Data. 


## Load Data

```{r setup, include=FALSE}

bp <- here::here("TidyTuesday_Explained/060-MLB_pitch_classification_8")

knitr::opts_chunk$set(echo = TRUE,
                      root.dir = bp)

suppressPackageStartupMessages({
  suppressWarnings({
    library(tidyverse)
    library(class)
    library(randomForest)
    library(caret)
    library(MLmetrics) # for log-loss
    library(measures) # for multi-class brier score
    library(pROC)
    library(xgboost)
    library(e1071)
    library(keras)
    library(doParallel)
  })
})

theme_set(theme_light())

set1 <- readRDS(file.path(bp,"2016_04_21_to_2016_04_23_pitch.rds"))
set2 <- readRDS(file.path(bp,"2016_04_24_to_2016_04_26_pitch.rds"))
train <- bind_rows(set1, set2)

#test
test <- readRDS(file.path(bp,"2016_04_27_to_2016_04_30_pitch.rds"))

#### Cleaning data based on EDA (below) ----------------------------------------------
train_cleaned <- train %>%
  filter(!pitch_type %in% c("KN", "IN", "PO", "EP", "FO"),
         !is.na(pitch_type)) %>%
  select(pitch_type, start_speed, end_speed, pfx_x, pfx_z, px, pz, x0, z0, vx0, vz0)

test_cleaned <- test %>%
  filter(!pitch_type %in% c("KN", "IN", "PO", "EP", "FO", "SC"),
         !is.na(pitch_type)) %>%
  select(pitch_type, start_speed, end_speed, pfx_x, pfx_z, px, pz, x0, z0, vx0, vz0)


train_cleaned$pitch_type <- as.factor(train_cleaned$pitch_type)
test_cleaned$pitch_type <- as.factor(test_cleaned$pitch_type)
```


## Helper Functions

Productionalize Model Building, Training and Testing

```{r}

model_summary <- function(model, class, class_probs, class_pred, param = list()){
  structure(
    list(
      class_actual = class, 
      class_prob = class_probs,
      class_pred = class_pred,
      model_type = class(model),
      param = param
      ),
    class = "fit_model_outputs"
  )
}

## Episode 54: bit.ly/TidyX_Ep54
data_scale <- function(train, test, ...){
  
  scale_by <- function(x, mean, sd){
    (x - mean) / sd
  }
  
  scale_values <- train %>% 
    select(-pitch_type) %>% 
    lapply(function(x){
      list(mean = mean(x), sd = sd(x))
    })
  
  ## prepare the training data
  train <- colnames(train) %>% 
    setdiff("pitch_type") %>%
    lapply(function(column_name){
      scale_by(
        x = train[[column_name]],
        mean = scale_values[[column_name]][["mean"]],
        sd = scale_values[[column_name]][["sd"]]
        )
      }) %>% 
    data.frame %>% 
    setNames(colnames(train) %>%  setdiff("pitch_type")) %>% 
    bind_cols(pitch_type = train$pitch_type)
  
  ## prepare the testing data
  test <- colnames(test) %>% 
    setdiff("pitch_type") %>%
    lapply(function(column_name){
      scale_by(
        test[[column_name]],
        scale_values[[column_name]][["mean"]],
        scale_values[[column_name]][["sd"]]
        )
      }) %>% 
    data.frame %>% 
    setNames(colnames(test) %>% setdiff("pitch_type")) %>% 
    bind_cols(pitch_type = test$pitch_type)
  
  list(
    train = train, 
    test = test
  )
}

## Episode 59: bit.ly/TidyX_Ep59
data_downsample <- function(train, test, seed, ...){
  set.seed(seed)
  
  train <- downSample(
    x = train %>% select(-pitch_type),
    y = train$pitch_type
    ) %>% 
    rename(
      pitch_type = Class
    )
    
  list(
    train = train, 
    test = test
  )
}

## Episode 59: bit.ly/TidyX_Ep59
data_upsample <- function(train, test, seed, ...){
  set.seed(seed)
  
  train <- upSample(
    x = train %>% select(-pitch_type),
    y = train$pitch_type
    ) %>% 
    rename(
      pitch_type = Class
    )
    
  list(
    train = train, 
    test = test
  )
}


## Evaluation Metrics

### Accuracy
eval_acc_overall <- function(x){
  stopifnot(class(x) == "fit_model_outputs")
  stopifnot(length(x$class_actual) == length(x$class_pred))
  sum(x$class_actual == x$class_pred) / length(x$class_actual)
}

eval_acc_class <- function(x){
  stopifnot(class(x) == "fit_model_outputs")
  stopifnot(length(x$class_actual) == length(x$class_pred))
  
  data.frame(
    class_actual = x$class_actual,
    class_pred = x$class_pred
    ) %>%
  count(class_pred, class_actual) %>%
  group_by(class_actual) %>%
  summarize(N = sum(n),
            matches = n[class_actual == class_pred],
            within_acc = matches / N)
}

## Episode 59: bit.ly/TidyX_Ep59
eval_logloss <- function(x){
  stopifnot(class(x) == "fit_model_outputs")
  stopifnot(length(x$class_actual) == nrow(x$class_prob))
  if(is.na(x$class_prob)){
    return(NA)
  }else{
    mlogloss <- MultiLogLoss(y_true = x$class_actual, y_pred = x$class_prob)
  }
  mlogloss
}

eval_multiclass_brier <- function(x){
  stopifnot(class(x) == "fit_model_outputs")
  stopifnot(length(x$class_actual) == nrow(x$class_prob))
  if(is.na(x$class_prob)){
    return(NA)
  }else{
    brier <- multiclass.Brier(truth = x$class_actual, probabilities = x$class_prob)
  }
  
  brier
}

eval_auc <- function(x){
  stopifnot(class(x) == "fit_model_outputs")
  stopifnot(length(x$class_actual) == length(x$class_pred))
  
  suppressMessages({
    auc <- multiclass.roc(response = x$class_actual,
                   predictor = as.numeric(x$class_pred))$auc
  })
  
  auc
}

```

## Model Training

### KNN  (TidyX Epsiode 54)

Train KNN using set params

```{r}

model_KNN <- function( train, test, ..., data_prep_function = list(scale_data)){
  
  if(!is.null(data_prep_function)){
    for(func_idx in seq_along(data_prep_function)){
      dat <- data_prep_function[[func_idx]](train, test, ...)
      train <- dat$train
      test <- dat$test
    }
  }
  
  ## train
  fit_knn <- knn(
    train = train[,setdiff(colnames(train),"pitch_type")],
    test = test[,setdiff(colnames(test),"pitch_type")],
    cl = train$pitch_type, 
    k = 4)
  
  ## outcomes
  class_pred <- factor(as.character(fit_knn),levels = levels(train$pitch_type))
  class_prob <- NA
  
  ## return
  model_summary(
    model = fit_knn,
    class = test$pitch_type,
    class_probs = class_prob,
    class_pred = class_pred,
    param = list(k=4)
    )
}

knn_standard <- model_KNN(train_cleaned, test_cleaned, data_prep_function = list(data_scale))
knn_standard_up <- model_KNN(train_cleaned, test_cleaned, data_prep_function = list(data_scale, data_upsample), seed = 123456)
knn_standard_down <- model_KNN(train_cleaned, test_cleaned, data_prep_function = list(data_scale, data_downsample), seed = 123456)

knn_standard_list <- list(knn_standard, knn_standard_up, knn_standard_down)

knn_standard_list %>%
  walk(eval_acc_overall)

knn_standard_list %>%
  walk(eval_acc_class)

saveRDS(knn_standard_list, file.path(bp,"knn_standard_models.rds"))

```

Train KNN - optimize

```{r}

model_KNN_caret <- function( train, test, ..., model_seed = 555, data_prep_function = list(scale_data)){
  
  if(!is.null(data_prep_function)){
    for(func_idx in seq_along(data_prep_function)){
      dat <- data_prep_function[[func_idx]](train, test, ...)
      train <- dat$train
      test <- dat$test
    }
  }
  
  set.seed(model_seed)
  ctrl <- trainControl(method="repeatedcv",repeats = 5)
  knnFit <- train(
    pitch_type ~ ., 
    data = train, 
    method = "knn",
    trControl = ctrl)
  
  ## outcomes
  class_pred <- predict(knnFit, newdata = test, type = "raw")
  class_prob <- predict(knnFit, newdata = test, type = "prob")
  
  ## return
  model_summary(
    model = knnFit,
    class = test$pitch_type,
    class_probs = class_prob,
    class_pred = class_pred,
    param = list(trainControl = list(method = "repeatedcv",repeats = 5))
    )
}

knn_caret <- model_KNN_caret(train_cleaned, test_cleaned, data_prep_function = list(data_scale))
knn_caret_up <- model_KNN_caret(train_cleaned, test_cleaned, data_prep_function = list(data_scale, data_upsample), seed = 123456)
knn_caret_down <- model_KNN_caret(train_cleaned, test_cleaned, data_prep_function = list(data_scale, data_downsample), seed = 123456)

knn_caret_list <- list(knn_caret, knn_caret_up, knn_caret_down)

knn_caret_list %>%
  walk(~print(eval_acc_overall(.x)))

knn_caret_list %>%
  walk(~print(eval_acc_class(.x)))

saveRDS(knn_caret_list, file.path(bp,"knn_caret_models.rds"))

```

### Random Forest (TidyX Epsiode 55)

Train RandomForest using set params

```{r}

model_rf <- function( train, test, ..., model_seed = 555, data_prep_function = NULL){
  if(!is.null(data_prep_function)){
    for(func_idx in seq_along(data_prep_function)){
      dat <- data_prep_function[[func_idx]](train, test, ...)
      train <- dat$train
      test <- dat$test
    }
  }
  
  set.seed(model_seed)
  
  ## train
  fit_rf <- randomForest(pitch_type ~ ., data = train, mtry = 6)
  
  ## return
  class_pred <- predict(fit_rf, newdata = test, type = "class")
  class_prob <- predict(fit_rf, newdata = test, type = "prob")
  
  ## return
  model_summary(
    model = fit_rf,
    class = test$pitch_type,
    class_probs = class_prob,
    class_pred = class_pred,
    param = list(mtry = 6)
    )
} 

rf_standard <- model_rf(train_cleaned, test_cleaned)
rf_standard_up <- model_rf(train_cleaned, test_cleaned, data_prep_function = list(data_upsample), seed = 123456)
rf_standard_down <- model_rf(train_cleaned, test_cleaned, data_prep_function = list(data_downsample), seed = 123456)

rf_standard_list <- list(rf_standard, rf_standard_up, rf_standard_down)

rf_standard_list %>%
  walk(~print(eval_acc_overall(.x)))

rf_standard_list %>%
  walk(~print(eval_acc_class(.x)))

saveRDS(rf_standard_list, file.path(bp,"rf_standard_models.rds"))

```

Train RandomForest - optimize

```{r}

model_rf_caret <- function( train, test, ..., model_seed = 555, data_prep_function = NULL){
  
  if(!is.null(data_prep_function)){
    for(func_idx in seq_along(data_prep_function)){
      dat <- data_prep_function[[func_idx]](train, test, ...)
      train <- dat$train
      test <- dat$test
    }
  }
  
  control <- trainControl(
    method = "repeatedcv",
    number = 5,
    repeats = 5,
    search = "grid")

  set.seed(model_seed)
  
  tunegrid <- expand.grid(.mtry=c(3:7))
  
  cl <- makePSOCKcluster(5)
  registerDoParallel(cl)
  
  ## train
  fit_rf <- train(pitch_type ~ .,
                     data = train,
                     method="rf",
                     metric = "Accuracy",
                     tuneGrid = tunegrid,
                     trControl = control)
  
  ## return
  class_pred <- predict(fit_rf, newdata = test, type = "raw")
  class_prob <- predict(fit_rf, newdata = test, type = "prob")
  
  ## return
  model_summary(
    model = fit_rf,
    class = test$pitch_type,
    class_probs = class_prob,
    class_pred = class_pred,
    param = list(trainControl = list(method = "repeatedcv", number = 5, repeats = 5, search = "grid"),
                 tunegrid = list(expand.grid = list(mtry = c(3:7))))
    )
} 

rf_caret <- model_rf_caret(train_cleaned, test_cleaned)
rf_caret_up <- model_rf_caret(train_cleaned, test_cleaned, data_prep_function = list(data_upsample), seed = 123456)
rf_caret_down <- model_rf_caret(train_cleaned, test_cleaned, data_prep_function = list(data_downsample), seed = 123456)

rf_caret_list <- list(rf_caret, rf_caret_up, rf_caret_down)

rf_caret_list %>%
  walk(~print(eval_acc_overall(.x)))

rf_caret_list %>%
  walk(~print(eval_acc_class(.x)))

saveRDS(rf_caret_list, file.path(bp,"rf_caret_models.rds"))

```

### XGBoost (TidyX Epsiode 56)

Train XGBoost using set params

```{r}

model_xgboost <- function( train, test, ..., model_seed = 555, data_prep_function = NULL){
  
  if(!is.null(data_prep_function)){
    for(func_idx in seq_along(data_prep_function)){
      dat <- data_prep_function[[func_idx]](train, test, ...)
      train <- dat$train
      test <- dat$test
    }
  }
  
  ## convert pitch type to numeric
  train$pitch_type_num <- as.numeric(train$pitch_type) - 1
  test$pitch_type_num <- as.numeric(test$pitch_type) - 1
  
  ## create training and test matrices
  train_x <- data.matrix(train %>% select(-pitch_type, -pitch_type_num))
  train_y <- train %>% pull(pitch_type_num)
  test_x <- data.matrix(test %>% select(-pitch_type, -pitch_type_num))
  test_y <- test %>% pull(pitch_type_num)
  
  ## store data in special XGBoost matrix
  train_xgb_matrix <- xgb.DMatrix(data = train_x, label = train_y)
  test_xgb_matrix <- xgb.DMatrix(data = test_x, label = test_y)
  
  set.seed(model_seed)
  
  ## train model
  number_pitches <- length(unique(train_cleaned$pitch_type))
  model_parameters <- list(
    "eval_metric" = "mlogloss",
    "num_class"   = number_pitches,
    "objective"   = "multi:softprob"
    )
  
  wl <- list(train = train_xgb_matrix, test = test_xgb_matrix)

  quiet <- capture.output({
  fit_xgb <- xgb.train(
    data = train_xgb_matrix,
    max.depth = 3,
    nrounds = 1000,
    watchlist = wl,
    nthreads = 3,
    early_stopping_rounds = 10,
    params = model_parameters,
    verbose = FALSE
  )})
  
  ## return
  class_prob <- predict(fit_xgb, newdata = test_xgb_matrix, reshape = TRUE)

  class_pred <- factor(
    levels(test$pitch_type)[max.col(class_prob, ties.method = "first")],
    levels = levels(test$pitch_type))
  
  ## return
  model_summary(
    model = fit_xgb,
    class = test$pitch_type,
    class_probs = class_prob,
    class_pred = class_pred,
    param = list(
      model_parameters = list(
        "eval_metric" = "mlogloss",
        "num_class"   = number_pitches,
        "objective"   = "multi:softprob"
      ),
      max.depth = 3,
      nrounds = 1000,
      watchlist = "train",
      nthreads = 3,
      early_stopping_rounds = 10
    ))
} 

xgb_standard <- model_xgboost(train_cleaned, test_cleaned)
xgb_standard_up <- model_xgboost(train_cleaned, test_cleaned, data_prep_function = list(data_upsample), seed = 123456)
xgb_standard_down <- model_xgboost(train_cleaned, test_cleaned, data_prep_function = list(data_downsample), seed = 123456)

xgb_standard_list <- list(xgb_standard, xgb_standard_up, xgb_standard_down)

xgb_standard_list %>%
  walk(~print(eval_acc_overall(.x)))

xgb_standard_list %>%
  walk(~print(eval_acc_class(.x)))

saveRDS(xgb_standard_list, file.path(bp,"xgb_standard_models.rds"))


```

Train XGBoost - optimize

```{r}

model_xgboost_caret <- function( train, test, ..., model_seed = 555, data_prep_function = NULL){
  
  if(!is.null(data_prep_function)){
    for(func_idx in seq_along(data_prep_function)){
      dat <- data_prep_function[[func_idx]](train, test, ...)
      train <- dat$train
      test <- dat$test
    }
  }
  
  set.seed(model_seed)
  
  ## train model
  cross_val_ctrl <- trainControl(method = "repeatedcv",
                        repeats = 1,
                        number = 3,
                        classProbs = TRUE,
                        allowParallel = TRUE)
  
  tune_grid <- expand.grid(nrounds = 1000,
                        eta = 0.01,
                        gamma = 0,
                        colsample_bytree = 1,
                        min_child_weight = 1,
                        subsample = 1,
                        max_depth = seq(2, 8, by = 2))
  
  cl <- makePSOCKcluster(7)
  registerDoParallel(cl)
  
  fit_xgb <- train(
    pitch_type ~ .,
    data = train,
    method = "xgbTree",
    trControl = cross_val_ctrl,
    tuneGrid = tune_grid,
    metric = "Accuracy",
    verbose = TRUE
  )
  
  ## return
  class_pred <- predict(fit_xgb, newdata = test, type = "raw")
  class_prob <- predict(fit_xgb, newdata = test, type = "prob")
  
  ## return
  model_summary(
    model = fit_xgb,
    class = test$pitch_type,
    class_probs = class_prob,
    class_pred = class_pred,
    param = list(
      trainControl = list(method = "repeatedcv",
                        repeats = 1,
                        number = 3,
                        classProbs = TRUE,
                        allowParallel = TRUE),
      tunegrid = list(nrounds = 1000,
                        eta = 0.01,
                        gamma = 0,
                        colsample_bytree = 1,
                        min_child_weight = 1,
                        subsample = 1,
                        max_depth = seq(2, 8, by = 2))
    ))
} 

xgb_caret <- model_xgboost_caret(train_cleaned, test_cleaned)
xgb_caret_up <- model_xgboost_caret(train_cleaned, test_cleaned, data_prep_function = list(data_upsample), seed = 123456)
xgb_caret_down <- model_xgboost_caret(train_cleaned, test_cleaned, data_prep_function = list(data_downsample), seed = 123456)

xgb_caret_list <- list(xgb_caret, xgb_caret_up, xgb_caret_down)

xgb_caret_list %>%
  walk(~print(eval_acc_overall(.x)))

xgb_caret_list %>%
  walk(~print(eval_acc_class(.x)))

saveRDS(xgb_caret_list, file.path(bp,"xgb_caret_models.rds"))

```


### Naive Bayes (TidyX Epsiode 57)

Train the Naive Bayes Algorithm using set params

```{r}

model_naive_bayes <- function( train, test, ..., model_seed = 555, data_prep_function = NULL){
  
  if(!is.null(data_prep_function)){
    for(func_idx in seq_along(data_prep_function)){
      dat <- data_prep_function[[func_idx]](train, test, ...)
      train <- dat$train
      test <- dat$test
    }
  }
  
  ## convert pitch type to numeric
  train$pitch_type_num <- as.numeric(train$pitch_type) - 1
  test$pitch_type_num <- as.numeric(test$pitch_type) - 1
  

  set.seed(model_seed)
  
  ## train model
  fit_nb <- naiveBayes(x = train %>% select(-pitch_type, -pitch_type_num),
                     y = train %>% select(pitch_type_num))
  
  ## return
  class_prob <- predict(fit_nb, newdata = test, type = "raw")
  class_pred <- factor(
    levels(test$pitch_type)[max.col(class_prob, ties.method = "first")],
    levels = levels(test$pitch_type))
  
  ## return
  model_summary(
    model = fit_nb,
    class = test$pitch_type,
    class_probs = class_prob,
    class_pred = class_pred,
    param = list())
} 

bayes_standard <- model_naive_bayes(train_cleaned, test_cleaned)
bayes_standard_up <- model_naive_bayes(train_cleaned, test_cleaned, data_prep_function = list(data_upsample), seed = 123456)
bayes_standard_down <- model_naive_bayes(train_cleaned, test_cleaned, data_prep_function = list(data_downsample), seed = 123456)

bayes_standard_list <- list(bayes_standard, bayes_standard_up, bayes_standard_down)

bayes_standard_list %>%
  walk(~print(eval_acc_overall(.x)))

bayes_standard_list %>%
  walk(~print(eval_acc_class(.x)))

saveRDS(bayes_standard_list, file.path(bp,"bayes_standard_models.rds"))


```

### Deep Learning (TidyX Epsiode 58)

Train a Neural Network  Algorithm using set params

```{r}

model_nn <- function( train, test, ..., model_seed = 555, data_prep_function = list(data_scale)){
  
  if(!is.null(data_prep_function)){
    for(func_idx in seq_along(data_prep_function)){
      dat <- data_prep_function[[func_idx]](train, test, ...)
      train <- dat$train
      test <- dat$test
    }
  }
  
  ## convert pitch type to categorical array
  train_pitch_type_num <- to_categorical(as.numeric(train$pitch_type)-1)
  test_pitch_type_num <- to_categorical(as.numeric(test$pitch_type)-1)
  
  train_x <- train %>%
    select(-pitch_type) %>%
    array() %>% 
    as.matrix
  
  test_x <- test %>%
    select(-pitch_type) %>%
    array() %>% 
    as.matrix
   

  ## Create model
  
  model <- keras_model_sequential()

  model %>%
    
    ## two hidden layers
    layer_dense(units = 128, activation = 'relu') %>%
    layer_dense(units = 128, activation = 'relu') %>%
    
    ## output layer
    layer_dense(units = 9, activation = 'softmax')
  
  
  ## define learning information and what model uses to figure out how to updae the weights next
  model %>% compile(
    optimizer = optimizer_adam(.01),
    loss = 'categorical_crossentropy',
    metrics = c('accuracy')
  )
  
  set.seed(model_seed)
  
  ## train model
  model %>%
    fit(
    x = train_x,
    y = train_pitch_type_num,
    validation_split = .2,
    epochs = 50,
    verbose = 0
  )
  
  ## return
  class_prob <- model %>% predict(test_x, type = "raw")
  class_pred <- class_prob %>% 
    data.frame() %>% 
    setNames(levels(train_cleaned$pitch_type)) %>% 
    mutate(
      idx = 1:nrow(.)
      ) %>% 
    pivot_longer(
      cols = 1:9,
      names_to = "type") %>% 
    group_by(idx) %>% 
    mutate(
      pred_pitch = type[which.max(value)]
      ) %>% 
    pivot_wider(
      names_from = type, 
      values_from = value
      ) %>% 
    ungroup() %>% 
    pull(pred_pitch)
  
  ## return
  model_summary(
    model = NA,
    class = test$pitch_type,
    class_probs = class_prob,
    class_pred = class_pred,
    param = list( hidden = list( dense = list(n = 2, units = 128, activation = "relu")),
                  output = list( dense = list(n = 1, units = 9, activation = "softmax")),
                  epochs = 50
                  ))
} 

nn_standard <- model_nn(train_cleaned, test_cleaned)
nn_standard_up <- model_nn(train_cleaned, test_cleaned, data_prep_function = list(data_scale, data_upsample), seed = 123456)
nn_standard_down <- model_nn(train_cleaned, test_cleaned, data_prep_function = list(data_scale, data_downsample), seed = 123456)

nn_standard_list <- list(nn_standard, nn_standard_up, nn_standard_down)

nn_standard_list %>%
  walk(~print(eval_acc_overall(.x)))

nn_standard_list %>%
  walk(~print(eval_acc_class(.x)))

saveRDS(nn_standard_list, file.path(bp,"nn_standard_models.rds"))

```

Deeper NN

```{r}

model_nn_deep <- function( train, test, ..., model_seed = 555, data_prep_function = list(data_scale)){
  
  if(!is.null(data_prep_function)){
    for(func_idx in seq_along(data_prep_function)){
      dat <- data_prep_function[[func_idx]](train, test, ...)
      train <- dat$train
      test <- dat$test
    }
  }
  
  ## convert pitch type to categorical array
  train_pitch_type_num <- to_categorical(as.numeric(train$pitch_type)-1)
  test_pitch_type_num <- to_categorical(as.numeric(test$pitch_type)-1)
  
  train_x <- train %>%
    select(-pitch_type) %>%
    array() %>% 
    as.matrix
  
  test_x <- test %>%
    select(-pitch_type) %>%
    array() %>% 
    as.matrix
   

  ## Create model
  
  model <- keras_model_sequential()

  model %>%
    
    ## two hidden layers
    layer_dense(units = 500, activation = 'relu') %>%
    layer_dense(units = 500, activation = 'relu') %>%
    layer_dense(units = 500, activation = 'relu') %>%
    
    ## output layer
    layer_dense(units = 9, activation = 'softmax')
  
  
  ## define learning information and what model uses to figure out how to updae the weights next
  model %>% compile(
    optimizer = optimizer_adam(.01),
    loss = 'categorical_crossentropy',
    metrics = c('accuracy')
  )
  
  set.seed(model_seed)
  
  ## train model
  model %>%
    fit(
    x = train_x,
    y = train_pitch_type_num,
    validation_split = .2,
    epochs = 100,
    verbose = 0
  )
  
  ## return
  class_prob <- model %>% predict(test_x, type = "raw")
  class_pred <- class_prob %>% 
    data.frame() %>% 
    setNames(levels(train_cleaned$pitch_type)) %>% 
    mutate(
      idx = 1:nrow(.)
      ) %>% 
    pivot_longer(
      cols = 1:9,
      names_to = "type") %>% 
    group_by(idx) %>% 
    mutate(
      pred_pitch = type[which.max(value)]
      ) %>% 
    pivot_wider(
      names_from = type, 
      values_from = value
      ) %>% 
    ungroup() %>% 
    pull(pred_pitch)
  
  ## return
  model_summary(
    model = NA,
    class = test$pitch_type,
    class_probs = class_prob,
    class_pred = class_pred,
    param = list( hidden = list( dense = list(n = 3, units = 500, activation = "relu")),
                  output = list( dense = list(n = 1, units = 9, activation = "softmax")),
                  epochs = 50
                  ))
} 

nn_deep <- model_nn_deep(train_cleaned, test_cleaned)
nn_deep_up <- model_nn_deep(train_cleaned, test_cleaned, data_prep_function = list(data_scale, data_upsample), seed = 123456)
nn_deep_down <- model_nn_deep(train_cleaned, test_cleaned, data_prep_function = list(data_scale, data_downsample), seed = 123456)

nn_deep_list <- list(nn_deep, nn_deep_up, nn_deep_down)

nn_deep_list %>%
  walk(~print(eval_acc_overall(.x)))

nn_deep_list %>%
  walk(~print(eval_acc_class(.x)))

saveRDS(nn_deep_list, file.path(bp,"nn_deep_models.rds"))

```
